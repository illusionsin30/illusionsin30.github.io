---
layout: home
title: Home
---

## 欢迎
这是主播在 Machine Learning 学习过程中做笔记的网站，目前会涉及到的方向主要是：

- 课程笔记：包括但不限于数理基础课程，计算机基础课程等.
- 论文阅读：部分前沿论文阅读，目前主要聚焦在 DiT 和 Linear Attention 上.
- 部分 machine learning 基础知识笔记
- $\cdots$

如果这个网页有帮助你，或者你想共同建设这个网页，可以通过网页中我的 github 主页以及个人邮箱 (gugusine773@gmail.com) 联系我.

## 笔记导航
### 学习笔记
#### Topology
[**Week11**: 同伦与基本群](/topology/TopoWeek11)
[**Week12**: 基本群与 $S^1$ 的基本群](/topology/TopoWeek12)
[**Week13**：$S^1$ 基本群、$S^n$ 基本群和 $T^2$ 基本群](/topology/TopoWeek13)
[**Week14**(待完善)](/topology/TopoWeek14)
[**Week15**](/topology/TopoWeek15)

#### Applied Stochastic Process
[**Chap1**: 概率论基础](/StocProcess/Chap1)
[**Chap6**: 连续参数 Markov 链(待更新)](/StocProcess/Chap6)

#### Introduction of Modern Biology
[**Final Review**(持续更新中...)](/Bio)

### Machine Learning 基础学习

[Transformer架构解读(待更新)](/ML/transformer)
[GPT与BERT架构(待更新)](/ML/GPTandBert)

### 前沿论文阅读笔记

#### 模型量化 Quantization
[\[**AAAI 2025 oral**\]MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models](/articles/MPQ-DM)
[\[**ECCV 2024**\]MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization](/articles/MixDQ)

#### 线性注意力 Linear attention
[\[**Google**\]Learning without training: The implicit dynamics of in-context learning](/articles/LWT)


目前很多笔记仍在进行中，主播努力学习ing...

