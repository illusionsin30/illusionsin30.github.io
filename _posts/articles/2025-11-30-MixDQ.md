---
layout: post
title: "[ECCV 2024]MixDQ"
date: 2025-11-30
permalink: /articles/MixDQ
toc: true
---

![MixDQ](../images/MixDQ.png)
[**MixDQ Github仓库**](https://github.com/thu-nics/MixDQ)

## 问题背景

这篇同样是与量化相关的文章，量化问题已经在上一篇 MPQ-DM 中有过详细讨论，这里不再细致讨论，仅说明 MixDQ 与 MPQ-DM 一致，都是在混合精度量化方向展开的工作.

## 论文创新

### 发现
模型不同层量化后对模型表现性能的影响不同，越靠前的层越容易导致模型表现性能大幅下降(表现为上下文语境改变或是图像质量下降)，MixDQ 论文中称这些层为 **高度敏感层**. 同时越多步数生成图像的模型对量化敏感度越低. 基于以上发现和文本 embeddings 的特征分布，MixDQ 提出一种 **BOS-aware** 的量化策略. 

![MixDQ-phe](../images/MixDQ-phe.png)

### 详细方法
依论文 Introduction 部分介绍，MixDQ 由三个内容组成：
- BOS-aware Text Embedding quantization
- Metric-decoupled sensitivity analysis
- integer-programming based mixed precision allocation

![MixDQ-method](../images/MixDQ-method.png)

输入文本经过 CLIP encoder 层后变为 text embedding 向量. 在 text embedding 中，MixDQ 发现第一个 token 总是有特别大的值 (823.5)，而其他 token 仅有 10-15 附近的值. 这是一个非常显著的 **outlier 异常值**现象，基于前一篇的讨论可知，这会严重影响 activation 量化的效果，让除第一个 token 以外的值都接近于 0，从而造成数据丢失. 

在 CLIP output 中，第一个 token 即是 tokenizer 中设置的 'Begin Of Sentence' token 即 `BOS_token`. 在不同提示词中 BOS token 的特征都一致，因而在量化计算时可以跳过这个 BOS token, 以保证量化计算的正确性. 

具体地，BOS-aware quantization 先**全精度计算**好 BOS token 的输出特征，然后将它拼接到其他 token **量化计算**的特征前. 这样 CLIP embedding 的量化 error 就被显著得降低，从而有效减缓模型精度下降问题. 由于这只是全精度计算一个 token，带来的空间开销与时间开销是可以接受的.

![MixDQ-BOS](../images/MixDQ-BOS.png)

**发现** 中对于层敏感性的问题仅是简单的定量分析(原文中使用特征值 L2 norm 与量化噪声的比值)，难以处理图像质量下降问题. MixDQ 需要有一种严谨的定量方法帮助分析敏感性的大小. 

Metric-decoupled sensitivity analysis 即为特意设计出的一类方法，其将指标与层解耦来分离量化对生成图像质量、上下文环境的影响. 具体地，这种方法将层分为两组，一组为上下文相关层 (FFNs, CrossAttention)，另一组为图像质量相关层 (self-attention, conv). 对质量相关层仍然采用 **SQNR 指标**. 对上下文相关层设计出 **Structural Similarity Index Measure (SSIM) 指标**：对生成图像 $x$ 与参考图像 $y$，设计以下量

$$
    l(x, y) = \frac{2\mu_x \mu_y}{\mu_x^2 + \mu_y^2}, c(x, y) = \frac{2\sigma_x \sigma_y}{\sigma_x^2 + \sigma_y^2}, s(x, y) = \frac{\sigma_{xy}}{\sigma_x \sigma_y}
$$

SSIM 指标出自 [\[IEEE TRAIP\]Zhou Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, *Image quality assessment: from error visibility to structural similarity*](https://ieeexplore.ieee.org/document/1284395), 其中对这三个量的解释分别为 **亮度 luminance**, **对比度 contrast**, **结构 structure**. SSIM 就可以表示为这三个量的幂乘积:

$$
    \mathrm{SSIM}(x,y) = l(x,y)^\alpha \cdot c(x, y)^\beta \cdot s(x, y)^\gamma
$$

原文设置了 $\alpha=\beta=\gamma$，即是将三个量直接相乘. 于是 SSIM 就可以接受图像结构化信息的改变，从而反映出两种图像上下文的不同.

基于以上两种策略，MixDQ 提出 **Integer Programming Bit-width Allocation** 的 layer-wise 混合精度量化方法. 对高敏感性层 (即量化后指标下降严重的层) 采用高 bit 量化，而其余精度采用正常量化. MixDQ 将这个量化问题建模为一个整数规划问题：给定资源预算 $\mathcal{B}$ 和备选量化 bit $b \in \\{ 2, 4, 8\\}$，需要决定各层的量化精度 $\mathcal{c}$ 让上述指标 (SSIM, SQNR) 最大化，并确保量化开销 $\mathcal{M}_{i, b}$ 控制在预算内.

$$
    \underset{c_{i, b}}{\argmax} \sum_{i=1}^N \sum_{b=2,4,8} c_{i, b} \mathcal{S}_{i, b} \\
    \text{s.t. } \sum_{b=2, 4, 8} c_{i, b} = 1, \sum_{i=1}^N \sum_{b=2,4,8} c_{i, b} \mathcal{M}_{i, b} \leq \mathcal{B}, c_{i, b} \in \{0, 1 \}
$$

结合量化会使指标 $\mathcal{S}_{i, b}$ 下降，因而这个整数规划就是选定低 bit 数量化的层，或者是说反向筛选全精度层让整体指标最大. 

看看原代码：
- BOS-aware text embedding quantization
```python
# thu-nics/MixDQ/quant_utils/qdiff/models/quant_block.py
class QuantAttnProcessor:
    r"""
    Default processor for performing attention-related computations.
    """
    def __init__(self, enable_bos_aware=False):
        self.enable_bos_aware = enable_bos_aware

    def __call__(
        self,
        attn: QuantAttention,
        hidden_states: torch.FloatTensor,
        encoder_hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        temb: Optional[torch.FloatTensor] = None,
        scale: float = 1.0,
    ) -> torch.Tensor:
        residual = hidden_states

        args = () if USE_PEFT_BACKEND else (scale,)

        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)

        input_ndim = hidden_states.ndim

        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)

        batch_size, sequence_length, _ = (
            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        )
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)

        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)

        query = attn.to_q(hidden_states, *args)

        if encoder_hidden_states is None:
            is_cross_attn = False
            encoder_hidden_states = hidden_states
        else:
            is_cross_attn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)

        self.split_first_token = False  # determine whether the attention layer have text embed as input
        # INFO: make cross_attn act_quant skip the 1st BoS token
        # only when cross_atttn, and to_k to_v need to be quantized
        # noted that when to_k have both w and a not quant, the input is placeholder, not actual prompt embedding
        # INFO: for lcm_lora, the attn.to_k is lora.Linear, the QuantLayer is the base_layer, and default_0,1
        if is_cross_attn:
            if (getattr(attn.to_k,'act_quant',False) or getattr(attn.to_k,'weight_quant',False)) or (getattr(attn.to_v,'act_quant',False) or getattr(attn.to_v,'weight_quant',False)):
                # assert ((attn.to_v.act_quant or attn.to_v.weight_quant) or (attn.to_k.act_quant or attn.to_k.weight_quant))
                self.split_first_token = True
            if getattr(attn.to_k,'base_layer',False):
                if getattr(attn.to_k.base_layer,'act_quant',False) or getattr(attn.to_k,'weight_quant',False):
                    self.split_first_token = True

        # INFO: split the 1st BoS token, use FP16 
        # (could be resolved by implicitly saving them during actual inference)
        if self.split_first_token and self.enable_bos_aware:
            key = attn.to_k(encoder_hidden_states[:,1:,:], *args)
            value = attn.to_v(encoder_hidden_states[:,1:,:], *args)

            # INFO: conduct inference of the first token as FP
            if getattr(attn.to_k,'base_layer',False):
                # the lora layer, store the existing quant state
                cur_weight_quant = attn.to_k.base_layer.weight_quant
                cur_act_quant = attn.to_k.base_layer.act_quant
                for block_ in [attn.to_k, attn.to_v]:
                    for layer_ in [block_.base_layer, block_.lora_A, block_.lora_B]:
                        layer_.weight_quant = False
                        layer_.act_quant = False
                key_first_token = attn.to_k(encoder_hidden_states[:,0,:].unsqueeze(1), *args)
                value_first_token = attn.to_v(encoder_hidden_states[:,0,:].unsqueeze(1), *args)
                for block_ in [attn.to_k, attn.to_v]:
                    for layer_ in [block_.base_layer, block_.lora_A, block_.lora_B]:
                        layer_.weight_quant = cur_weight_quant
                        layer_.act_quant = cur_act_quant

            else: # normal layer, simply call the FP fwd_func of QuantLayer
                key_first_token = attn.to_k.fwd_func(encoder_hidden_states[:,0,:].unsqueeze(1), attn.to_k.org_weight, attn.to_k.org_bias, **attn.to_k.fwd_kwargs)
                value_first_token = attn.to_v.fwd_func(encoder_hidden_states[:,0,:].unsqueeze(1), attn.to_v.org_weight, attn.to_v.org_bias, **attn.to_v.fwd_kwargs)

            key = torch.cat([key_first_token,key],dim=1)
            value = torch.cat([value_first_token,value],dim=1)
        else:
            key = attn.to_k(encoder_hidden_states, *args)
            value = attn.to_v(encoder_hidden_states, *args)

        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)

        attention_probs = attn.get_attention_scores(query, key, attention_mask)

        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)

        # linear proj
        hidden_states = attn.to_out[0](hidden_states, *args)
        # dropout
        hidden_states = attn.to_out[1](hidden_states)

        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)

        if attn.residual_connection:
            hidden_states = hidden_states + residual

        hidden_states = hidden_states / attn.rescale_output_factor

        return hidden_states
```

- Metric-decoupled sensitivity analysis
```python
# 首先是 SQNR 指标相关
# thu-nics/MixDQ/quant_utils/qdiff/analysis_tools/error_func/quant_error.py
def LossFunction(pred, tgt, grad=None):
    """
    Compute the quant error: MES and the SQNR
    """
    # MSE Loss
    mse = lp_loss(pred, tgt, p=2, reduction='all')
    
    # SQNR
    err = pred - tgt
    # tensor1 和 tensor2 的后三个维度的元素求平方和
    tgt = torch.sum(tgt**2)  #or tgt = torch.norm(tgt)**2
    err = torch.sum(err**2)  #or err = torch.norm(err)**2
    # 二者相除
    divided = tgt / err
    # 直接计算信噪比
    sqnr = 10*torch.log10(divided)

    # SQNR
    # another settings
    # err = pred - tgt
    # # tensor1 和 tensor2 的后三个维度的元素求平方和
    # tgt = torch.sum(tgt**2, dim=(1,2,3))  #or tgt = torch.norm(tgt)**2
    # err = torch.sum(err**2, dim=(1,2,3))  #or err = torch.norm(err)**2
    # # 二者相除
    # divided = tgt / err
    # # 最后对第一个维度（也就是batch）求平均
    # sqnr = 10*torch.log10(torch.mean(divided))
    return mse, sqnr
```
```python
# 然后是 SSIM 指标相关
# thu-nics/MixDQ/quant_utils/qdiff/analysis_tools/error_func/quant_content.py
# Layer wise的敏感度分析，此时没有产生敏感度列表，应该遍历layer
def SSIM_Layer(model, qnn, pipe, opt, prompts, weight_quant=True, act_quant=False, weight_only=True, progressivly=False, config_ssim_layer={}, prefix=""):
    '''
    weight only: if to quantize the weight only
    progressivly: if False, quantize the only one layer in the model
    '''
    for name, module in model.named_children():
        full_name = prefix + name if prefix else name
        # logger.info(f"{name} {)}")
        if isinstance(module, QuantLayer):
            # TODO:可以手工filter掉自己想要或者不想要的层
            # if 'ff' in full_name or 'attn2' in full_name:
            # set_quant_state(module, weight_quant=weight_quant, act_quant=act_quant)
            module.set_quant_state(weight_quant=weight_quant, act_quant=act_quant)
            logger.info(f"{full_name}: weight_quant={weight_quant}, act_quant={act_quant}")
            with torch.no_grad():
                sample_fid(prompts, qnn, pipe, opt, batch_size=32, quant_inference = True, is_fp16= False)

            ssim = SSIM(img_path1=opt.image_folder, 
                        img_path2='/home/fangtongcheng/fast_ptq/diffuser-dev/analysis_tools/error_func/ssim_sensitivity/fp32_imgs', 
                        bs=32)

            logger.info('SSIM:{:.5f}\n'.format(float(ssim)))

            config_ssim_layer[full_name] = float(ssim)

            if weight_only:
                if not progressivly:
                    qnn.set_quant_state(False, False)
            else:
                if not progressivly:
                    qnn.set_quant_state(True, False)

        else:
            SSIM_Layer(model=module, qnn=qnn, pipe=pipe, opt=opt, prompts=prompts, weight_quant=weight_quant, act_quant=act_quant, weight_only=True, progressivly=False, config_ssim_layer=config_ssim_layer, prefix=full_name+".")
```
