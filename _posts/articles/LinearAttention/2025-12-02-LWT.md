---
layout: post
title: "[Google]Learning without training"
date: 2025-12-02
permalink: /articles/LWT
toc: true
---

## Intro
在一般的部署流程中，大语言模型仅在 pre-training 和 finetune 阶段修正内部参数的值，即 “学习” 过程. 在大模型正式开启 inference 或其他任务时模型的内部参数会被冻结，模型仅能依赖输入的 prompt 来适配不同的任务. 这让模型在不同任务上的表现差距较大，且对于未参与过训练的内容性能表现大幅下降.  

**In-Context-Learning (ICL)** 则是指 LLM 在完全训练完成后依然能够在上下文中学习的能力. Google 提出 contextual block 的概念，其通过一个简单的 contextual layer (训练学习上下文信息的层) 叠加一个神经网络构成. Google 的这篇论文以一个简单的 vanilla transformer 架构为例，从理论上证明上下文信息能够隐性转换为对特定 contextual layers 的**权重更新**，即隐性的 fine-tuning 过程. 

## 理论证明

### Contextual Blocks
Contextual Block 由一个 contextual layer 和一个标准神经网络组成. 首先需要明确 contextual layer 的概念，

> Contextual layer：一个神经网络（映射） $A(\cdot)$，可以接受**单个向量** $x$ 为输入，输出 $A(x)$，或是接受一个**上下文** $C$ 与一个向量 $x$ 拼接后得到的输入 $[C, x]$，输出 $A([C, x])$，简记为 $A(C, x)$. 上下文 $C$ 可以是 tokens 序列，也可以是一张图片. 

Contextual layer 的原型是 transformer block 中的 self-attetion 层，因而可以直接类比考虑. 在 LLM transformer self-attention中，上下文 $C$ 是 prompt tokens 序列，可以写为 $C = [c_1, \cdots, c_n]$，输入向量 $x$ 即是 query token. 那么 self-attention 的输出就可以用上述表示方法表示为 $A(C, x)$，且容易得到 $A(x)$ 与 $A(C, x)$ 是占据同样的输出向量空间 (这从映射的角度很容易理解). 