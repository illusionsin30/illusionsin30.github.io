---
layout: post
title: "[Google]Learning without training"
date: 2025-12-02
permalink: /articles/LWT
toc: true
---

## Intro
在一般的部署流程中，大语言模型仅在 pre-training 和 finetune 阶段修正内部参数的值，即 “学习” 过程. 在大模型正式开启 inference 或其他任务时模型的内部参数会被冻结，模型仅能依赖输入的 prompt 来适配不同的任务. 这让模型在不同任务上的表现差距较大，且对于未参与过训练的内容性能表现大幅下降.  

**In-Context-Learning (ICL)** 则是指 LLM 在完全训练完成后依然能够在上下文中学习的能力. Google 提出 contextual block 的概念，其通过一个简单的 contextual layer (训练学习上下文信息的层) 叠加一个神经网络构成. Google 的这篇论文以一个简单的 vanilla transformer 架构为例，从理论上证明上下文信息能够隐性转换为对特定 contextual layers 的**权重更新**，即隐性的 fine-tuning 过程. 

## 理论证明

### Contextual Blocks
Contextual Block 由一个 contextual layer 和一个标准神经网络组成. 首先需要明确 **contextual layer** 的概念，

> **Contextual layer** 一个神经网络（映射） $A(\cdot)$，可以接受**单个向量** $x$ 为输入，输出 $A(x)$，或是接受一个**上下文** $C$ 与一个向量 $x$ 拼接后得到的输入 $[C, x]$，输出 $A([C, x])$，简记为 $A(C, x)$. 上下文 $C$ 可以是 tokens 序列，也可以是一张图片. 

Contextual layer 的原型是 transformer block 中的 self-attetion 层，因而可以直接类比考虑. 在 LLM transformer self-attention中，上下文 $C$ 是 prompt tokens 序列，可以写为 $C = [c\_1, \cdots, c\_n]$，输入向量 $x$ 即是 query token. 那么 self-attention 的输出就可以用上述表示方法表示为 $A(C, x)$，且容易得到 $A(x)$ 与 $A(C, x)$ 是占据同样的输出向量空间 (这从映射的角度很容易理解). 定义 contextual layer 的输出为 contextual vectors, 而后就可以定义是否带 context 的层输出的差别 $\Delta A(C) = A(C, x) - A(x)$. 

明确 contextual layer 的概念，就可以定义 **contextual block**：

> **Contextual block** 一个复合映射 $T\_W = M\_W \circ A$，其中 $A$ 为 contextual layer, $M_W$ 为一个神经网络映射. 简单而言 $M\_W(z) = f\_\theta(Wz+b)$，$W, b$ 分别为第一个全连接层的权重矩阵和偏置，$f\_\theta$ 表示神经网络剩余结构.

Contextual block 可以将 context 中的一部分 $Y \subset C$ 转换为神经网络权重的更新 $W \leftarrow W + \Delta W(Y)$，即 $Y$ 中的信息被传递为 $\Delta W(Y)$，输入 contextual block 的向量就可以变为 $[C \backslash Y, x]$. 以下定理给出，这种权重更新的方法与全 context 的方法是等价的.

> **Theorem 2.2.** 考虑一个 context block $T_W = M_W \circ A$，其中 $M_W$ 是权重矩阵为 $W$ 的全连接层. 给定 context $C$ 和输入 $x$，$C$ 中一部分 $Y \subset C$ 对 $T_W$ 输出的的作用效果可以隐性对应 $M_W$ 第一层秩为 1 的权重更新 $\Delta W(Y)$. 即
> 
> $$
>   T_W(C, x) = T_{W+\Delta W(Y)}(C \backslash Y, x), \text{ where } \Delta W(Y) = \frac{(W \Delta A(Y))A(C \backslash Y, x)^T}{\parallel A(C \backslash Y, x) \parallel^2}
> $$
> 
> 其中 $\Delta A(Y) = A(C, x) - A(C \backslash Y, x)$ 是与 $Y$ 相关的 context vector，$\Delta W(Y)$ 秩为 1. 

证明很直接，直接按以上定义形式代入

$$
\begin{aligned}
    T_{W + \Delta W(Y)}(C \backslash Y, x) &= M_{W + \Delta W(Y)}(A(C \backslash Y, x)) \\ 
    &= f_\theta \left( (W + \Delta W(Y))A(C \backslash Y, x) + b\right) \\
    &= f_\theta \left( WA(C \backslash Y, x) + \Delta W(Y)A(C \backslash Y, x) + b \right)
\end{aligned}
$$

根据**最小二乘法策略**(或者叫秩1更新 rank-one update)，希望让更新后的权重作用在 $A(C \backslash Y, x)$ 能够与原权重作用在 $A(C, x)$ 上的效果相同，即

$$
    (W + \Delta W(Y))A(C \backslash Y, x) = W A(C, x)
$$

定义 $\Delta A(Y) = A(C, x) - A(C \backslash Y, x)$ ，有

$$
    \Delta W(Y) A(C \backslash Y, x) = W \Delta A(Y)
$$

这是一个**欠定问题**，其标准形式类似于 $\Delta W k = v$. 这里希望找到二次范数最小的 $\Delta W(Y)$ 解，使得这个更新过程对模型改变最小，即优化问题

$$
    \underset{\Delta W(Y)}{\min} \frac{1}{2}\parallel \Delta W \parallel^2_F \\
    \text{s.t. } \Delta W(Y) A(C \backslash Y, x) = W \Delta A(Y)
$$

由拉格朗日乘子法可以得到该问题的解为

$$
\Delta W(Y) = \frac{(W\Delta A(Y)) A(C \backslash Y, x)^T}{\parallel A(C \backslash Y, x)\parallel^2}
$$

代入原始式可得

$$
\begin{aligned}
    T_{W + \Delta W(Y)} (C \backslash Y, x) &= f_\theta \left( WA(C \backslash Y, x) + \frac{(W\Delta A(Y)) A(C \backslash Y, x)^T}{\parallel A(C \backslash Y, x)\parallel^2}A(C \backslash Y, x) + b \right) \\
    &= f_\theta \left(WA(C \backslash Y, x) + W\Delta A(Y) + b\right) \\
    &= f_\theta \left( WA(C, x) + b \right) \\
    &= T_{W} (C, x)
\end{aligned}
$$

于是在这种更新机制下 contextual block 得到的输出与原始输出是等价的. $\mathrm{Q. E. D.}$.

如果取 $Y = C$，即将完整的 context $C$ 都转换为权重 $W$ 的更新 $\Delta W(C)$，那么就可以得到更加好用的一个推论

> **Corollary 2.3.1.** 完整上下文 $C$ 可以经以下更新过程转换为神经网络的权重
>
>$$
>   T_W(C, x) = T_{W + \Delta W(C)}(x), \text{ where } \Delta W(C) = \frac{(w \Delta A)A(x)^T}{\parallel A(x) \parallel^2}
>$$
>
>其中 $\Delta W$ 秩为 1，由于 $W\Delta A$ 是一个列向量而 $A(x)^T$ 是一个行向量.

这个推论从理论上说明大模型 ICL 的可行性. 原论文在 Appendix A 中给出了这个定理在 skip-connections 中的正确性，解释清晰该方法在目前 transformer block 中的可行性.

### The implicit learning dynamics of ICL
