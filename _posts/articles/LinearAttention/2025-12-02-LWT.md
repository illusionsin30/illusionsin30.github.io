---
layout: post
title: "[Google]Learning without training"
date: 2025-12-02
permalink: /articles/LWT
toc: true
---

## Intro
在一般的部署流程中，大语言模型仅在 pre-training 和 finetune 阶段修正内部参数的值，即 “学习” 过程. 在大模型正式开启 inference 或其他任务时模型的内部参数会被冻结，模型仅能依赖输入的 prompt 来适配不同的任务. 这让模型在不同任务上的表现差距较大，且对于未参与过训练的内容性能表现大幅下降.  

**In-Context-Learning (ICL)** 则是指 LLM 在完全训练完成后依然能够在上下文中学习的能力. Google 提出 contextual block 的概念，其通过一个简单的 contextual layer (训练学习上下文信息的层) 叠加一个神经网络构成. Google 的这篇论文以一个简单的 vanilla transformer 架构为例，从理论上证明上下文信息能够隐性转换为对特定 contextual layers 的**权重更新**，即隐性的 fine-tuning 过程. 

## 理论证明

### Contextual Blocks
Contextual Block 由一个 contextual layer 和一个标准神经网络组成. 首先需要明确 **contextual layer** 的概念，

> **Contextual layer** 一个神经网络（映射） $A(\cdot)$，可以接受**单个向量** $x$ 为输入，输出 $A(x)$，或是接受一个**上下文** $C$ 与一个向量 $x$ 拼接后得到的输入 $[C, x]$，输出 $A([C, x])$，简记为 $A(C, x)$. 上下文 $C$ 可以是 tokens 序列，也可以是一张图片. 

Contextual layer 的原型是 transformer block 中的 self-attetion 层，因而可以直接类比考虑. 在 LLM transformer self-attention中，上下文 $C$ 是 prompt tokens 序列，可以写为 $C = [c\_1, \cdots, c\_n]$，输入向量 $x$ 即是 query token. 那么 self-attention 的输出就可以用上述表示方法表示为 $A(C, x)$，且容易得到 $A(x)$ 与 $A(C, x)$ 是占据同样的输出向量空间 (这从映射的角度很容易理解). 定义 contextual layer 的输出为 contextual vectors, 而后就可以定义是否带 context 的层输出的差别 $\Delta A(C) = A(C, x) - A(x)$. 

明确 contextual layer 的概念，就可以定义 **contextual block**：

> **Contextual block** 一个复合映射 $T\_W = M\_W \circ A$，其中 $A$ 为 contextual layer, $M_W$ 为一个神经网络映射. 简单而言 $M\_W(z) = f\_\theta(Wz+b)$，$W, b$ 分别为第一个全连接层的权重矩阵和偏置，$f\_\theta$ 表示神经网络剩余结构.

Contextual block 可以将 context 中的一部分 $Y \subset C$ 转换为神经网络权重的更新 $W \leftarrow W + \Delta W(Y)$，即 $Y$ 中的信息被传递为 $\Delta W(Y)$，输入 contextual block 的向量就可以变为 $[C \backslash Y, x]$. 以下定理给出，这种权重更新的方法与全 context 的方法是等价的.

> **Theorem 2.2.** 考虑一个 context block $T_W = M_W \circ A$，其中 $M_W$ 是权重矩阵为 $W$ 的全连接层. 给定 context $C$ 和输入 $x$，$C$ 中一部分 $Y \subset C$ 对 $T_W$ 输出的的作用效果可以隐性对应 $M_W$ 第一层秩为 1 的权重更新 $\Delta W(Y)$. 即
> 
> $$
>   T_W(C, x) = T_{W+\Delta W(Y)}(C \backslash Y, x), \text{ where } \Delta W(Y) = \frac{(W \Delta A(Y))A(C \backslash Y, x)^T}{\parallel A(C \backslash Y, x) \parallel^2}
> $$
> 
> 其中 $\Delta A(Y) = A(C, x) - A(C \backslash Y, x)$ 是与 $Y$ 相关的 context vector，$\Delta W(Y)$ 秩为 1. 

