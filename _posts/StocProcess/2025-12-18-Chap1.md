---
layout: post
title: Chapter1-Probability Theory
permalink: /StocProcess/Chap1
toc: true
---

应用随机过程基于概率论，因而我们需要先学习概率论相关内容. 然而概率论内容实在是太多太杂，这里仅介绍与随机过程紧密相关的一些概率论概念与定理.

## 矩母函数与拉普拉斯变换
### 矩母函数
矩母函数 (moment generating function) 的定义基于随机变量 $X$ 的**矩** (moment) 的定义，先简单介绍一下矩的定义：

> **Definition 1.1 (矩)** 设 $X$ 为一随机变量，记
> 
> $$
>   E(X^k) = \int_{-\infty}^{+\infty} x^k \mathrm{d} F_X(x), k \in \mathbb{N}
> $$ 
> 
> 为 $X$ 的 **$k$ 阶矩**.

离散情况下以上定义自然退化为 $E(X^k) = \sum\_{i=0}^\infty x\_iP(X=x\_i)$. 从矩母函数的名称 Moment **generating** function 就可以知道，这个函数是用来生成随机变量的矩的，并且可以生成任意 $k$ 阶矩. 考虑 $k$ 阶矩的形式以及随机变量和的期望所满足的性质，不难想到用指数函数的方式定义矩母函数是比较合理的：

> **Definition 1.2(矩母函数)** 设 $X$ 为任意随机变量，若积分
> 
> $$
>   \psi(t) := E \left( e^{tX} \right) = \int_{-\infty}^{+\infty} e^{tX} \mathrm{d}F_X(x)
> $$
> 
> 存在，则称 $\psi(t)$ 为 $X$ 的**矩母函数**.

不难验证，积分存在的情况下对 $\psi(t)$ 在 $t=0$ 点做 Taylor 展开，

$$
    \psi(t) = E \left(e^{tX} \right) = E \left( 1 + Xt + \frac{X^2}{2}t^2 + \cdots \right) = 1 + tE(X) + t^2 \frac{E(X^2)}{2} + \cdots
$$

自然有 $\psi^{(k)}(0) = E(X^k)$. 不难发现矩母函数与分布函数是一一对应关系，即 $\psi\_{X\_1}(t) = \psi\_{X\_2}(t) \Leftrightarrow F_{X\_1}(x) = F_{X\_2}(x)$. 

设 $X\_1, X\_2$ 相互独立，且他们的矩母函数分别为 $g\_{X\_1}(t), g\_{X\_2}(t)$，$X\_1 + X\_2$ 的矩母函数就为

$$
\begin{aligned}
    g_{X_1 + X_2}(t) &= E\left(e^{t(X_1 + X_2)}\right) \\
    &= E \left( e^{tX_1} e^{tX_2} \right) \\
    &= E \left( e^{tX_1}\right)E \left( e^{tX_2}\right) \\
    &= g_{X_1}(t)g_{X_2}(t)
\end{aligned}
$$

矩母函数可直接与期望和方差关联：
- $E(X) = \psi^{(1)}(0)$
- $Var(X) = \psi^{(2)}(0) - \left( \psi^{(1)}(0) \right)^2$

### 拉普拉斯-斯蒂尔切斯变换
有时直接用随机变量的分布函数计算会很麻烦，在数学侧已经有拉普拉斯这位大神给出了一套解决方法：将分布函数变换到另一个空间中计算，再将结果变换回来. 

> **Definition 1.3 (拉普拉斯-斯蒂尔切斯变换)** 设 $X$ 为非负随机变量，分布函数为 $F\_X(x)$，$s=a+bi, a > 0, b \in \mathbb{R}$，称积分
> 
> $$
>   \hat{F}_X(s) = \int_0^{\infty} e^{-sx}\mathrm{d}F_X(x)
> $$
> 
> 为 $F\_X(x)$ 的 **拉普拉斯-斯蒂尔切斯变换** (Laplace-Stieltjes transform)，简称 $X$ 的 L-S 变换.

L-S 变换能够保证 $\hat{F}\_X(s)$ 与 $F\_X(x)$ 具有一一对应关系，且容易看出对存在 L-S 变换的独立随机变量 $X\_1, X\_2$ 也有

$$
    \hat{F}_{X_1 + X_2}(s) = \hat{F}_{X_1}(s)\hat{F}_{X_2}(s)
$$

## 条件数学期望
条件数学期望建立在条件概率之上，先来了解条件概率：

> **Definition 2.1 (条件概率)** 设 $A, B$ 为两个随机事件，且 $P(B) > 0$，称 $P(A \| B) = P(AB) / P(B)$ 为事件 $B$ 发生时，事件 $A$ 的 **条件概率**.

基于这个定义，很容易写出离散型随机变量 $X, Y$ 的条件概率表达：

$$
    P(X = x_i | Y = y_j) = \frac{P(X = x_i, Y = y_j)}{P(Y = y_j)}
$$

这也被称为给定 $Y = y_j$ 时 $X$ 的条件分布律. 固定 $Y = y_j$ 就很好写出 $X$ 的期望

$$
    E(X | Y = y_j) = \sum_{i} x_i P(X = x_i | Y = y_j)
$$

依据全概率公式，就可以给出条件数学期望的定义：

> **Definition 2.2 (离散随机变量的条件数学期望)** 设 $X, Y$ 为离散型随机变量，称
> 
> $$
>   E(X | Y) := \sum_{j} E(X | Y = y_j) P(Y = y_j)
> $$
> 
> 为 $X$ 关于 $Y$ 的**条件数学期望**，简称条件期望.

不难推广到连续型随机变量中，将求和转化为**积分**，离散型概率转变为**概率密度函数**即可得到连续随机变量的条件数学期望：

> **Definition 2.3 (连续随机变量的条件数学期望)** 设 $X, Y$ 为连续型随机变量，联合概率密度函数为 $f(x, y)$，称
> 
> $$
>   E(X | Y) := \int_{-\infty}^{+\infty} xf_{X | Y}(x | y) \mathrm{d} x
> $$
> 
> 为 $X$ 关于 $Y$ 的**条件数学期望**，简称条件期望. 其中 $X$ 的条件概率密度函数 $f\_{X \| Y}(x \| y)$ 定义如下
> 
> $$
>   f_{X | Y}(x | y) = \frac{f(x, y)}{f_{Y}(y)}
> $$
> 
> Y 的概率密度函数 $f\_{Y}(y) = \int\_{-\infty}^{\infty} f(x, y)\mathrm{d} x$.

基于以上分析，条件数学期望 $E(X \| Y)$ 实质上是关于 $y$ 的函数，即 $E(X \| Y) = g(y)$. 因而，**条件数学期望不是期望，而是随机变量**. 既然条件数学期望是随机变量，那么他也自然可以求期望以及条件数学期望：

> **Proposition 2.1**: 设 $X, Y$ 均为随机变量，$X$ 关于 $Y$ 的条件期望存在且为 $E(X \| Y)$，记 $\mathcal{B}$ 为 $Y$ 样本空间互不相容事件的集合，满足 $\Omega\_Y = \underset{i}{\cup} B_i, B_i \in \mathcal{B}$. 对任意 $D \in \mathcal{B}$ 有
> 
> $$
>   E\left[ E(X | Y) | Y \in D \right] = E(X | Y \in D)
> $$

这个结论很好证明，基于全概率公式，将以上条件期望用定义式展开

$$
    E\left[ E(X | Y) | Y \in D \right] = \int_{y \in D} E(X | Y = y)f_{Y}(y) \mathrm{d} y = \int_{y \in D} x f(x, y) \mathrm{d}(y) = E(X | Y \in D)
$$

如果取 $D = \mathbb{R}$，以上公式就等价为以下式子：

$$
    E[E(X | Y) | Y] = \int_{-infty}^\infty E(X | Y) f_{Y}(y) \mathrm{d} y = E(X)
$$

这是一个不错的结果，意味着条件期望的条件期望不再是一个随机变量，而是一个数. 这也为我们计算一个随机变量的期望提供新的视角，后续会经常用到这个式子. 多元随机变量的讨论与二元没有本质区别，因而就止步于此. 基于以上重要的概率论概念，以及其他有关的数字特征 (如期望、方差、协方差等)，就可以正式迈入随机过程的学习.

## 例题
这一部分的例题主要用于复习概率论知识，因而先从一些简单的概率论证明入手.

> **证明以下命题**：若事件序列 $\\{ A\_n, n\geq 1 \\}$ 是单调增序列，则 
> 
> $$
>   \lim_{n \to \infty} P(A_n) = P \left( \lim_{n \to \infty} A_n\right)
> $$

依 $\\{ A\_n \\}$ 序列满足单调增，可构造相应的互斥事件序列 $\\{ B\_n \\}$ 满足

$$
    B_n = \begin{cases} 
        A_1, & n = 1 \\
        A_n - A_{n-1} & n \geq 2
    \end{cases}
$$

则 $A\_n$ 的概率可以表示为

$$
    P(A_n) = \sum_{i=1}^n P(B_n) = P \left( \underset{i=1}{\overset{n}{\bigcup}} B_i \right) = P \left( \underset{i=1}{\overset{n}{\bigcup}} A_i \right)
$$

取极限 $n \to \infty$ 很容易得到

$$
    \lim_{n \to \infty} P(A_n) = P \left( \lim_{n \to \infty} \underset{i=1}{\overset{n}{\bigcup}} A_i \right) = P \left( \lim_{n \to \infty} A_n \right)
$$

这个结论对于单调减序列也成立，不过此时的证明过程需要搬到 $A\_n^c$ 中进行。

